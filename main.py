import pandas as pd
import numpy as np
import re
import os
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# sklearn æ ¸å¿ƒåº“
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

warnings.filterwarnings('ignore')

plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class CompanyTypeClassifier:
    def __init__(self, file_path):
        self.file_path = file_path
        self.df = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.best_model = None
        self.model_filename = 'best_company_classifier_pipeline.pkl'

    def _clean_text(self, text):
        """
        æ–‡æœ¬æ¸…æ´—å‡½æ•°
        1. è½¬å­—ç¬¦ä¸²
        2. å»é™¤ç‰¹æ®Šç¬¦å·ï¼Œä»…ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
        3. å­—ç¬¦é—´åŠ ç©ºæ ¼ï¼ˆæ¨¡æ‹Ÿåˆ†è¯ï¼Œæ–¹ä¾¿TF-IDFå¤„ç†ï¼‰
        """
        if pd.isna(text):
            return ""
        text = str(text).strip()
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return ' '.join(list(text))

    def load_and_preprocess(self):
   
        print("\n" + "="*50)
        print(">>> æ­¥éª¤ 1: æ•°æ®åŠ è½½ä¸æ¸…æ´—")
        print("="*50)

        # 1.1 æ™ºèƒ½åŠ è½½ CSV æˆ– Excel
        try:
            if self.file_path.endswith('.csv'):
                self.df = pd.read_csv(self.file_path)
            else:
                self.df = pd.read_excel(self.file_path)
            
            # 1.2 ç»Ÿä¸€åˆ—å (å‡è®¾ç¬¬ä¸€åˆ—æ˜¯æ ‡ç­¾ï¼Œç¬¬äºŒåˆ—æ˜¯æ–‡æœ¬)
            cols = self.df.columns
            if len(cols) >= 2:
                self.df = self.df.rename(columns={cols[0]: 'label', cols[1]: 'text'})
                self.df = self.df[['label', 'text']] # åªä¿ç•™å‰ä¸¤åˆ—
            
            print(f"åŸå§‹æ•°æ®é‡: {len(self.df)}")

            # 1.3 å»é™¤ç©ºå€¼
            self.df.dropna(subset=['text', 'label'], inplace=True)
            
            # 1.4 åº”ç”¨æ–‡æœ¬æ¸…æ´—
            print("æ­£åœ¨æ¸…æ´—æ–‡æœ¬...")
            self.df['clean_text'] = self.df['text'].apply(self._clean_text)
            
            # å»é™¤æ¸…æ´—åä¸ºç©ºçš„è¡Œ
            self.df = self.df[self.df['clean_text'].str.len() > 0]
            
            print(f"æœ‰æ•ˆæ•°æ®é‡: {len(self.df)}")
            print(f"ç±»åˆ«åˆ†å¸ƒ:\n{self.df['label'].value_counts().sort_index()}")
            
        except FileNotFoundError:
            print(f"âŒ é”™è¯¯: æ–‡ä»¶ {self.file_path} æœªæ‰¾åˆ°ã€‚")
            raise
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½é”™è¯¯: {e}")
            raise

    def split_data(self):
        """æ­¥éª¤2ï¼šæ•°æ®åˆ’åˆ† (80% è®­ç»ƒ, 20% éªŒè¯)"""
        print("\n" + "="*50)
        print(">>> æ­¥éª¤ 2: æ•°æ®åˆ’åˆ† (é˜²æ­¢æ•°æ®æ³„éœ²)")
        print("="*50)

        # ä½¿ç”¨ stratify ä¿è¯è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒä¸€è‡´
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.df['clean_text'], 
            self.df['label'], 
            test_size=0.2, 
            random_state=42, 
            stratify=self.df['label']
        )
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(self.X_train)}")
        print(f"éªŒè¯é›†å¤§å°: {len(self.X_test)}")

    def train_and_optimize(self):
        """æ­¥éª¤3ï¼šæ¨¡å‹è®­ç»ƒä¸è¶…å‚æ•°ä¼˜åŒ–"""
        print("\n" + "="*50)
        print(">>> æ­¥éª¤ 3: æ¨¡å‹è®­ç»ƒä¸äº¤å‰éªŒè¯")
        print("="*50)

        # å®šä¹‰ Pipeline
        # è¯´æ˜ï¼šPipeline å°†å‘é‡åŒ–å’Œåˆ†ç±»å™¨æ‰“åŒ…ï¼Œfitæ—¶åªè®¡ç®—è®­ç»ƒé›†æ•°æ®çš„TF-IDFï¼Œé˜²æ­¢æ³„éœ²
        pipeline = Pipeline([
            ('tfidf', TfidfVectorizer(
                token_pattern=r'(?u)\b\w+\b', 
                max_features=5000,
                ngram_range=(1, 2)  # å…³é”®ï¼šåŒæ—¶æå–å•å­—å’ŒåŒå­—è¯ç»„ç‰¹å¾
            )),
            ('clf', SVC(class_weight='balanced', probability=True)) # é»˜è®¤ä½¿ç”¨ SVM
        ])

        # å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´
        param_grid = [
            {
                'clf': [SVC(class_weight='balanced', probability=True, kernel='linear')],
                'clf__C': [1, 10],  # æƒ©ç½šç³»æ•°
                'tfidf__max_features': [3000, 5000]
            },
            {
                'clf': [LogisticRegression(class_weight='balanced', max_iter=1000)],
                'clf__C': [1, 10],
                'tfidf__max_features': [3000, 5000]
            }
        ]

        print("å¼€å§‹ç½‘æ ¼æœç´¢ (GridSearchCV) å¯»æ‰¾æœ€ä½³æ¨¡å‹...")
        # 5æŠ˜äº¤å‰éªŒè¯
        grid_search = GridSearchCV(
            pipeline, 
            param_grid, 
            cv=5, 
            scoring='accuracy',
            n_jobs=-1, # å¹¶è¡Œè®¡ç®—
            verbose=1
        )
        
        grid_search.fit(self.X_train, self.y_train)
        
        self.best_model = grid_search.best_estimator_
        
        print(f"\nâœ… äº¤å‰éªŒè¯æœ€ä½³å‡†ç¡®ç‡: {grid_search.best_score_:.4f}")
        print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        
        # æ£€æŸ¥æ˜¯å¦è¾¾æ ‡
        if grid_search.best_score_ < 0.8:
            print("âš ï¸ æ³¨æ„: è®­ç»ƒé›†äº¤å‰éªŒè¯å‡†ç¡®ç‡ç•¥ä½äº 80%ï¼Œåç»­å¯èƒ½éœ€è¦å¢åŠ æ•°æ®æˆ–è°ƒæ•´ç‰¹å¾ã€‚")
        else:
            print("âœ… è®­ç»ƒé˜¶æ®µæŒ‡æ ‡è¾¾æ ‡ (>=80%)ã€‚")

    def evaluate_final(self):
        """æ­¥éª¤4ï¼šæœ€ç»ˆéªŒè¯é›†è¯„ä¼°"""
        print("\n" + "="*50)
        print(">>> æ­¥éª¤ 4: æœ€ç»ˆéªŒè¯é›† (Hold-out Test) è¯„ä¼°")
        print("="*50)
        
        # é¢„æµ‹
        y_pred = self.best_model.predict(self.X_test)
        
        # è®¡ç®—æŒ‡æ ‡
        acc = accuracy_score(self.y_test, y_pred)
        
        print(f"æµ‹è¯•é›†å‡†ç¡®ç‡ (Accuracy): {acc:.4f}")
        print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(self.y_test, y_pred))
        
        # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        plt.figure(figsize=(12, 10))
        cm = confusion_matrix(self.y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title(f'æ··æ·†çŸ©é˜µ (Accuracy: {acc:.2%})')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.tight_layout()
        plt.show()
        
        if acc >= 0.8:
            print(f"\nğŸ‰ æ­å–œï¼æ¨¡å‹æœ€ç»ˆå‡†ç¡®ç‡ {acc:.2%} >= 80%ï¼Œç¬¦åˆäº¤ä»˜æ ‡å‡†ã€‚")
            return True
        else:
            print(f"\nâš ï¸ æ¨¡å‹æœ€ç»ˆå‡†ç¡®ç‡ {acc:.2%} æœªè¾¾åˆ° 80%ã€‚å»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡æˆ–å°è¯•æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚")
            return False

    def save_pipeline(self):
        """æ­¥éª¤5ï¼šä¿å­˜æ¨¡å‹ç®¡é“"""
        print("\n" + "="*50)
        print(">>> æ­¥éª¤ 5: ä¿å­˜æ¨¡å‹ä¸äº¤ä»˜")
        print("="*50)
        
        joblib.dump(self.best_model, self.model_filename)
        print(f"âœ… æ¨¡å‹å…¨æµç¨‹ç®¡é“å·²ä¿å­˜è‡³: {self.model_filename}")
        print("è¯¥æ–‡ä»¶åŒ…å«ï¼šé¢„å¤„ç†è§„åˆ™ + TF-IDFå‘é‡åŒ–å™¨ + è®­ç»ƒå¥½çš„åˆ†ç±»å™¨")

    def predict_new_data(self, texts):
        """å¯¹å¤–æ¥å£ï¼šé¢„æµ‹æ–°æ–‡æœ¬"""
        print("\n>>> æ¨¡æ‹Ÿæ–°æ•°æ®é¢„æµ‹:")
        
        # åŠ è½½æ¨¡å‹ (å¦‚æœæ˜¯é‡æ–°è¿è¡Œè„šæœ¬)
        if self.best_model is None:
            try:
                self.best_model = joblib.load(self.model_filename)
            except:
                print("æœªæ‰¾åˆ°ä¿å­˜çš„æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒã€‚")
                return

        # æ¸…æ´—è¾“å…¥
        clean_texts = [self._clean_text(t) for t in texts]
        
        # é¢„æµ‹ (Pipeline ä¼šè‡ªåŠ¨å¤„ç† TF-IDF)
        preds = self.best_model.predict(clean_texts)
        probs = self.best_model.predict_proba(clean_texts)
        
        for i, (text, pred, prob) in enumerate(zip(texts, preds, probs)):
            print(f"-"*30)
            print(f"æ–‡æœ¬: {text[:30]}...")
            print(f"é¢„æµ‹ç±»åˆ«: {pred}")
            print(f"ç½®ä¿¡åº¦: {np.max(prob):.4f}")

# ==========================================
# ä¸»ç¨‹åºæ‰§è¡Œå…¥å£
# ==========================================
if __name__ == "__main__":
    # 1. è®¾ç½®æ–‡ä»¶è·¯å¾„ (è¯·ç¡®ä¿ training.xlsx æˆ– training.csv åœ¨åŒçº§ç›®å½•)
    DATA_FILE = 'training.xlsx' 
    
 
    if not os.path.exists(DATA_FILE) and not os.path.exists('training.csv'):
        print("âš ï¸ æœªæ£€æµ‹åˆ°æ•°æ®æ–‡ä»¶ï¼Œæ­£åœ¨ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºä»£ç æµ‹è¯•...")
        dummy_data = {
            'Column1': np.random.randint(1, 12, 200),
            'Column2': ['æŸæŸç§‘æŠ€å…¬å¸ä¸“æ³¨äºè½¯ä»¶å¼€å‘ ' + str(i) for i in range(200)]
        }
        pd.DataFrame(dummy_data).to_excel(DATA_FILE, index=False)
    
    # 2. å®ä¾‹åŒ–å·¥ä½œæµ
    classifier = CompanyTypeClassifier(DATA_FILE)
    
    try:
        # 3. ä¾æ¬¡æ‰§è¡Œä»»åŠ¡
        classifier.load_and_preprocess() # åŠ è½½æ¸…æ´—
        classifier.split_data()          # åˆ’åˆ†æ•°æ®
        classifier.train_and_optimize()  # è®­ç»ƒä¼˜åŒ–
        classifier.evaluate_final()      # æœ€ç»ˆè¯„ä¼°
        classifier.save_pipeline()       # ä¿å­˜æ¨¡å‹
        
        # 4. æ¼”ç¤ºé¢„æµ‹
        test_samples = [
            "æœ¬å…¬å¸ä¸“ä¸šä»äº‹æˆ¿åœ°äº§å¼€å‘ä¸ç‰©ä¸šç®¡ç†æœåŠ¡ï¼Œè‡´åŠ›äºæ‰“é€ é«˜ç«¯ä½å®…ã€‚",
            "å…¬å¸ä¸»è¦ä¸šåŠ¡ä¸ºè½¯ä»¶æŠ€æœ¯å¼€å‘ã€äº’è”ç½‘ä¿¡æ¯æœåŠ¡åŠå¤§æ•°æ®åˆ†æã€‚",
            "æä¾›ä¸“ä¸šçš„é‡‘èæŠ•èµ„å’¨è¯¢ã€è‚¡æƒç§å‹ŸåŠèµ„äº§ç®¡ç†æœåŠ¡ã€‚"
        ]
        classifier.predict_new_data(test_samples)
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œä¸­æ–­: {e}")
        import traceback
        traceback.print_exc()